\documentclass[compress,red]{beamer}
%\documentclass[handout]{beamer}

%\usepackage{beamerthemesplit}
\usepackage{fancyvrb}
\usepackage{graphicx}
\usepackage{gb4e}
\usepackage{qtree}
\usepackage{hyperref}
\usepackage{ulem}

\usepackage{amsmath,amssymb,amsfonts}

\setbeamerfont{page number in head/foot}{size=\large}
\setbeamertemplate{footline}[frame number]

%\setbeamertemplate{footline}%
%{%
%\hfill\insertpagenumber\ of \ref{TotPages}\hspace{.5cm}\vspace{.5cm}
%\hfill\insertpagenumber\ of 28\hspace{.5cm}\vspace{.5cm}
%}%



\mode<presentation>
{
%\usetheme{Singapore}
%\usetheme{Berlin}

%\setbeamercovered{transparent}

}


%\mode<handout>
%{
%\usepackage{pgfpages}
%\pgfpagesuselayout{4 on 1}[a4paper,landscape,border shrink=5mm]
%}


\usetheme{Antibes}
%\usecolortheme{beetle}
%\usecolortheme{seagull}
\usecolortheme{lily}

\title[Lecture 3]{Statistical methods for linguistic research: Foundational Ideas}

\author{Shravan Vasishth}

\institute{Universit\"at Potsdam\\
vasishth@uni-potsdam.de\\
http://www.ling.uni-potsdam.de/$\sim$vasishth
}

\date{\today}

\begin{document}
\maketitle



<<setup,include=FALSE,cache=FALSE>>=
library(knitr)
library(coda)

# set global chunk options, put figures into folder
options(replace.assign=TRUE,show.signif.stars=FALSE)
opts_chunk$set(fig.path='figures/figure-', fig.align='center', fig.show='hold')
options(replace.assign=TRUE,width=75)
opts_chunk$set(dev='postscript')
#library(rstan)
#set.seed(9991)
# save workspace image, if you want
#the.date <- format(Sys.time(), "%b%d%Y")
#save.image(file=paste0("homework01-",the.date,".RData")
@



\section{The story so far}

\begin{frame}[fragile]\frametitle{Summary}
\begin{enumerate}
\item We learnt about the single sample, two sample, and paired t-tests.
\item We learnt about Type I, II error (and power).
\item We learnt about Type M and Type S errors.
\end{enumerate}

Now we are ready to look at linear modeling.

\end{frame}

\section{Linear models}

\subsection{Example: the noise and degree experiment}

\begin{frame}[fragile]\frametitle{Maxwell and Delaney dataset}

\begin{enumerate}
\item
Here's a simplified data set from Maxwell and Delaney's book (Designing Expts and Analyzing Data). 
\item
This is \textbf{within-subjects} (fake) experimental data; it has a $2\times 2$ design. 
\item 
Within-subjects designs have the property that each subject sees every condition in the experiment. 
\item Contrast this to \textbf{between}-subjects designs, e.g., medical trials with treatment vs placebo conditions. 
\end{enumerate}

\end{frame}

\begin{frame}[fragile]\frametitle{Maxwell and Delaney dataset}

Imagine that subjects are shown a stimulus (a picture) on the screen, and it's either shown with no noise (distortion, say) in the background, or with noise; in addition, the stimulus was either horizontal (tilted 0 degrees), or tilted by 4 degrees.  

The experiment has two levels of noise, and two levels of tilt.

\end{frame}

\begin{frame}[fragile]\frametitle{Load the data}

<<>>=
noisedeg<-read.table("data/noisedeg.txt",header=TRUE)
head(noisedeg)
@

\end{frame}


\begin{frame}[fragile]\frametitle{Load the data}

Let's compute the means by factor levels:

<<>>=
means<-with(noisedeg,tapply(rt,IND=list(noise,deg),
                            mean))
means
@

\end{frame}

\begin{frame}[fragile]\frametitle{Paired t-test on the data}


This is how one would do a t-test with such data, to compare means across (sets of) conditions:

<<>>=
no.noise<-subset(noisedeg,noise=="no.noise")
no.noise.means<-with(no.noise,tapply(rt,subj,mean))

noise<-subset(noisedeg,noise=="noise")
noise.means<-with(noise,tapply(rt,subj,mean))
@

\end{frame}

\begin{frame}[fragile]\frametitle{Paired t-test on the data}

<<>>=
no.noise.means
noise.means
@

\end{frame}

\begin{frame}[fragile]\frametitle{Paired t-test on the data}

<<>>=
t.test(noise.means,no.noise.means,paired=TRUE)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Paired t-test on the data}

Note that the order in which we compare the two vectors does not matter:

<<>>=
t.test(noise.means,no.noise.means,
       paired=TRUE)$statistic
t.test(no.noise.means,noise.means,
       paired=TRUE)$statistic
@

All that changes is the sign of the t-value.

\end{frame}


\begin{frame}[fragile]\frametitle{Paired t-test on the data}


These are the means we are comparing in the noise case:

<<>>=
## means of noise levels:
with(noisedeg,tapply(rt,noise,mean))
## mean of no.noise=500
## mean of noise=500+138=638
@

\end{frame}

\begin{frame}[fragile]\frametitle{Paired t-test on the data}

And here are the means we would compare in the degree case if we were to do pairwise t-tests on the different levels (0 vs 4 deg):

<<>>=
with(noisedeg,tapply(rt,deg,mean))
## mean of 0 deg=477
## mean of 4 deg=477+108=585
@

As for noise, we could fit t-tests repeatedly for degree as well.  (Try this yourself later.)

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

Now consider this \textbf{linear model}, which evaluates rt as a function of noise (\textbf{Note: this model is incorrect for the present dataset, but it's useful to understand how it work in order to explain how linear mixed models work}):

<<>>=
m0<-lm(rt~noise,noisedeg)
round(summary(m0)$coefficients,digits=2)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

It's important to understand what lies behind this output. 

We won't go through everything in the output, but only focus on some aspects that are immediately of relevance to us. First, we have the \textbf{coefficients}, the intercept and slope of the fitted line respectively:

<<>>=
coef(m0)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

One instructive exercise is to compare these coefficients with the means we have for noise. The mean of no.noise is 486, and the mean of noise is 486+90=576:

<<>>=
## means of noise levels:
with(noisedeg,tapply(rt,noise,mean))
## mean of no.noise=486
## mean of noise=486+90=576
@

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

\begin{enumerate}
\item
The intercept is giving us the mean of the no-noise condition.
\item 
The slope is giving us the difference in means between no-noise and noise.
\end{enumerate}

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

It is an assumption of the linear model that the residuals are (approximately) normally distributed.

We can check that this assumption is met:

<<>>=
## residuals:
res.m0<-residuals(m0)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

Plot the residuals by comparing them to a normal distribution: 

<<echo=FALSE,fig.height=4>>=
library(car)
qqPlot(res.m0)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

Next, we will look at the matrix formulation of the linear model.

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

Underlyingly, we have a design matrix or model matrix, which is being used by R to estimate the coefficients:

<<>>=
head(model.matrix(m0),n=7)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

Our linear model equation for the noise model m0 above is a system of equations. The single equation:

\begin{equation} \label{eq1a}
Y_i = \beta_{0} + \beta_{1}X_i + \epsilon_i 
\end{equation}

can be expanded to:

\begin{equation} \label{matrixeq1}
 \begin{array}{ccccccc}
Y_1    & = & \beta_0 & + & X_1 \beta_1 & + & \epsilon_1 \\
Y_2    & = & \beta_0 & + & X_2 \beta_1 & + & \epsilon_2 \\
Y_3    & = & \beta_0  & + & X_3 \beta_1 & + & \epsilon_3 \\
Y_4    & = & \beta_0 & + & X_4 \beta_1 & + & \epsilon_4 \\
\vdots &   & \vdots  &   & \vdots      &   & \vdots  \\
Y_n    & = & \beta_0 & + & X_n \beta_1 & + & \epsilon_n \\
\end{array} 
\end{equation}

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

And this system of linear equations can be restated in compact matrix form:

\begin{equation}
\mathbf{Y} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}
\end{equation}

where

\textbf{Vector of responses}:

\begin{equation} \label{matrixsum2}
\mathbf{Y} = \left( \begin{array}{c}
Y_1 \\
Y_2 \\
Y_3 \\
Y_4 \\
\vdots \\
Y_n \\
\end{array} \right)
\end{equation}

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

The design matrix (in R this is called the model matrix):

\begin{equation} \label{matrixsum3}
\mathbf{X} = \left( \begin{array}{cc}
1 & 0 \\
1 & 0 \\
1 & 0 \\
\vdots & \vdots \\
1 & 1 \\
1 & 1 \\
1 & 1 \\
\end{array} \right)
\end{equation}

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

Vector of parameters to be estimated:

\begin{equation} \label{matrixsum1}
\mathbf{\beta} = \left( \begin{array}{c}
\beta_0 \\
\beta_1 \\
\end{array} \right)
\end{equation}

and 

Vector of error terms (residuals):

\begin{equation} \label{matrixsum}
\mathbf{\epsilon} = \left( \begin{array}{c}
\epsilon_1 \\
\epsilon_2 \\
\epsilon_3 \\
\epsilon_4 \\
\vdots \\
\epsilon_n \\
\end{array} \right)
\end{equation}

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

We could write the whole equation as:

\begin{equation} \label{matrixsum0}
\left( \begin{array}{c}
Y_1 \\
Y_2 \\
Y_3 \\
Y_4 \\
\vdots \\
Y_n \\
\end{array} \right)
=
\left( \begin{array}{cc}
1 & X_1 \\
1 & X_2 \\
1 & X_3 \\
1 & X_4 \\
\vdots & \vdots \\
1 & X_n \\
\end{array} \right) 
\times 
\left( \begin{array}{c}
\beta_1 \\
\beta_2 \\
\end{array} \right)
+
\left( \begin{array}{c}
\epsilon_1 \\
\epsilon_2 \\
\epsilon_3 \\
\epsilon_4 \\
\vdots \\
\epsilon_n \\
\end{array} \right)
\end{equation}

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

\begin{enumerate}
\item
Our main goal when we fit a linear model is to find estimates of the parameters $\beta_0$ and $\beta_1$, the intercept and slope respectively.
\item
We will call the estimates $\hat{\beta}_0$ and $\hat{\beta}_1$. 
\item
This can be done by ``solving'' for the beta's. X is the model matrix, and X$'$ is the transpose of the model matrix. Y is the vector of dependent variables.
\end{enumerate}

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

\begin{equation}
\beta=(X'X)^{-1} X'Y 
\end{equation}

You do not need to know how the above equation comes about; all you need to know is that given the design matrix X, and the data Y, we can estimate the parameters.

In case you are interested in more detail, see the derivation these lecture notes:
https://github.com/vasishth/LM

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}
\framesubtitle{Contrast coding}

Returning to our noise and deg(ree) example above, we can also fit a linear model where we examine the effect of deg on rt.

Here, a critical thing to attend to is the \textbf{contrast coding} for the factor degree:

<<>>=
noisedeg$deg<-factor(noisedeg$deg)
contrasts(noisedeg$deg)
@

The above contrast coding says the following: compare 0 with deg 4. This kind of contrast is called treatment contrast coding: there's always a baseline that you compare another condition with. 

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

Let's fit the model:

<<>>=
## evaluating effect of degree:
round(summary(m1<-lm(rt~deg,noisedeg))$coefficients,digits=3)
## compare the coefficients with means computed earlier
@

Compare the coefficients with these means for deg:

<<>>=
with(noisedeg,tapply(rt,deg,mean))
## mean of 0 deg=477
## mean of 4 deg=477+108=585
@

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

We can also examine the effects of noise and degree together:

<<>>=
## evaluating effect of noise AND degree together:
m2<-lm(rt~noise+deg,noisedeg)
round(summary(m2)$coefficients,digits=3)
@

\end{frame}

\subsection{Continuous predictors}

\begin{frame}[fragile]\frametitle{Linear models}

\begin{enumerate}
\item
In our current noise and deg example, the `predictors' are categorial ones. 
\item
What about when we have continuous predictors?
\item
For example, we have instructors' ``beauty'' levels measured on a continuous scale as predictors of their teaching evaluations.
\item 
Beauty levels are centered; this means that a beauty level of 0 means average beauty level. This is a data set from a paper by 
Hamermesh and Parker
(Beauty in the Classroom: Instructors' Pulchritude and Putative Pedagogical Productivity," Economics of Education Review, August 2005). I got the data from Gelman and Hill (2007).
\end{enumerate}

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

<<>>=
bdata <- read.table("data/beauty.txt",header=T)
head(bdata)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

Note that the beauty scores are centered to have mean (approximately) 0:

<<>>=
summary(bdata$beauty)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}


One model we can fit:

$y = \beta_0 + \epsilon$

<<>>=
m2<-lm(evaluation~1,bdata)
mean(bdata$evaluation)
round(summary(m2)$coefficients,digits=3)
@

This model is only estimating the grand mean of evaluation scores.

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

$y = \beta_0 + \beta_1 x + \epsilon$

<<>>=
m3<-lm(evaluation~beauty,bdata)
round(summary(m3)$coefficients,digits=3)
@

The intercept now means: the expected evaluation score given an average beauty level.

The slope means: the expected increase in evaluation with \textbf{one unit} increase in beauty.

\end{frame}

\section{Summary so far}
\begin{frame}[fragile]\frametitle{Summary}

We now know how to fit

\begin{enumerate}
\item
Simple linear models with categorical predictors (the noise and degree data)
\item 
Simple linear models with continuous predictors.
\end{enumerate}

For more sophisticated models, see the Linear Modeling notes.

Next, we will discuss the last topic of this course, linear mixed models.

\end{frame}



\end{document}


